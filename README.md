# ğŸ½ï¸ Food Inspection Analysis

## ğŸ“Œ Project Overview

This project presents a comprehensive food safety inspection analytics platform for restaurants and other food establishments in **Chicago** and **Dallas**, developed using publicly available government datasets. The objective is to empower public health officials, analysts, and decision-makers with actionable insights into:

- Inspection outcomes  
- Risk categories  
- Violation patterns  

By leveraging the **medallion architecture (bronze â†’ silver â†’ gold)** for data organization and analytics, the platform delivers clean, reliable, and structured data pipelines that drive impactful visualizations and decision support.

---

## ğŸ› ï¸ Tools & Technologies

- **Azure Data Factory** â€“ Orchestration for data cleaning  
- **Azure Data Lake** â€“ Raw and curated data storage (bronze & silver layers)  
- **ER Studio** â€“ Dimensional data modeling
- **Databricks** â€“ Data Transformation and staging using PySpark notebooks  
- **Snowflake** â€“ Data warehousing and analytical querying (gold layer)  
- **Tableau** â€“ Dashboards for data visualization and insights  
- **Microsoft Excel** â€“ Initial data exploration

---

## ğŸ—‚ï¸ Datasets Overview

### **Chicago Food Inspections**
- Inspections of restaurants and other food establishments conducted by the **Chicago Department of Public Healthâ€™s Food Protection Program**
- Reviewed by licensed LEHPs (State of Illinois)
- [ğŸ”— Link to Dataset](#)

### **Dallas Food Inspections**
- Conducted by the **Dallas Code Compliance Services Department**
- [ğŸ”— Link to Dataset](#)

---

## ğŸ—ï¸ Architecture: Medallion Layers

- Raw Source Files (CSV) â†“ [Bronze Layer] â†’ Ingested raw data (Azure Data Lake) â†“ [Silver Layer] â†’ Cleaned,  deduplicated parquet files (Databricks) â†“ [Gold Layer] â†’ Transformed facts and dimensions (Snowflake) 

---

## ğŸš€ How to Run the Pipeline

### 1. **Data Ingestion & Cleaning**
- Use **Azure Data Factory (ADF)** pipeline to:
  - Extract raw data from source containers
  - Clean and store as Parquet files in the **Silver Layer**

### 2. **Data Transformation**
- Open Databricks and run the following notebooks in order:

#### ğŸ”¹ `Chicago_Data_Transformation.ipynb`
- Reads Chicago Parquet from silver layer
- Applies transformations, standardization, flatten
- Writes to **stg_chicago table** in Snowflake

#### ğŸ”¹ `Dallas_Data_Transformation.ipynb`
- Similar processing for Dallas
- Outputs to **stg_dallas table** in Snowflake

#### ğŸ”¹ `Merge_Dallas_Chicago_Data.ipynb`
- Combines both staging tables
- Adds `source` column to identify origin
- Generates UUID-based `job_id` and date for `load_dt`
- Loads to the **final combined stage table** in Snowflake: `stg_final_table`

### 3. **Data Loading (Fact & Dim Model)**
- From the combined stage table, load into Snowflake **dimensions and facts**:
- Run the `Dynamic_Tables_Query.sql` to load data into dimensions and facts

---

## ğŸ“Š Tableau Dashboards

- **Average Violations by Category**  
- **High-Risk Establishments Map View**  
- **Trend of Inspections by Year**  
- **Violation Type Frequency Heatmap**

These dashboards help surface hotspots, compliance patterns, and risk zones to city officials.

---

## ğŸ” Key Highlights

- Built with **modular, scalable architecture** using medallion pattern  
- All records tagged with `job_id`, `load_dt`, and `source` for auditability  

---

## ğŸ‘¥ Authors

- **Muskan Deepak Raisinghani** 
- **Rachana Keshav** 
- **Rishabh Shah** 

